{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c9044a-f479-4838-87cc-f340bac7dfbd",
   "metadata": {},
   "source": [
    "UDACITY SageMaker Essentials: Training Job Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ce7a25-49fa-41fb-88e0-e41381902b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257e6c4-b50b-4189-b15a-426a0ef3b7b9",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "\n",
    "The data we'll be examining today is a collection of reviews for an assortment of toys and games found on Amazon. This data includes, but is not limited to, the text of the review itself as well as the number of user \"votes\" on whether or not the review was helpful. Today, we will be making a model that predicts the usefulness of a review, given only the text of the review. This is an example of a problem in the domain of supervised sentiment analysis; we are trying to extract something subjective from text given prior labeled text.\n",
    "\n",
    "Before we get started, we want to know what form of data is accepted in the algorithm we're using. We'll be using BlazingText, an implemention of Word2Vec optimized for SageMaker. In order for this optimization to be effective, data needs to be preprocessed to match the correct format. The documentation for this algorithm can be found here: https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\n",
    "\n",
    "We will be training under \"File Mode\", which requires us to do two things in preprocessing this data. First, we need to generate labels from the votes. For this exercise, if the majority of votes for a review is helpful, we will designate it __label__1, and if the majority of votes for a review is unhelpful, we will designate it __label__2. In the edge case where the values are equal, we will drop the review from consideration. Second, we need to separate the sentences, while keeping the original label for the review. These reviews will often consist of several sentences, and this algorithm is optimized to perform best on many small sentences rather than fewer larger paragraphs. We will separate these sentences by the character \".\"\n",
    "\n",
    "(This process is obviously very naive, but we will get remarkable results even without a lot of finetuning!)\n",
    "\n",
    "This preprocessing is done for you in the cells below. Make sure you go through the code and understand what's being done in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70069d6a-8111-4155-87c7-3f71747167f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Function below unzips the archive to the local directory. \n",
    "\n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')\n",
    "\n",
    "# Input data is a file with a single JSON object per line with the following format: \n",
    "# {\n",
    "#  \"reviewerID\": <string>,\n",
    "#  \"asin\": <string>,\n",
    "#  \"reviewerName\" <string>,\n",
    "#  \"helpful\": [\n",
    "#    <int>, (indicating number of \"helpful votes\")\n",
    "#    <int>  (indicating total number of votes)\n",
    "#  ],\n",
    "#  \"reviewText\": \"<string>\",\n",
    "#  \"overall\": <int>,\n",
    "#  \"summary\": \"<string>\",\n",
    "#  \"unixReviewTime\": <int>,\n",
    "#  \"reviewTime\": \"<string>\"\n",
    "# }\n",
    "# \n",
    "# We are specifically interested in the fields \"helpful\" and \"reviewText\"\n",
    "#\n",
    "\n",
    "def label_data(input_data):\n",
    "    labeled_data = []\n",
    "    HELPFUL_LABEL = \"__label__1\"\n",
    "    UNHELPFUL_LABEL = \"__label__2\"\n",
    "     \n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        reviewText = l_object['reviewText']\n",
    "        if total_votes != 0:\n",
    "            if helpful_votes / total_votes > .5:\n",
    "                labeled_data.append(\" \".join([HELPFUL_LABEL, reviewText]))\n",
    "            elif helpful_votes / total_votes < .5:\n",
    "                labeled_data.append(\" \".join([UNHELPFUL_LABEL, reviewText]))\n",
    "          \n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "# Labeled data is a list of sentences, starting with the label defined in label_data. \n",
    "\n",
    "def split_sentences(labeled_data):\n",
    "    split_sentences = []\n",
    "    for d in labeled_data:\n",
    "        label = d.split()[0]        \n",
    "        sentences = \" \".join(d.split()[1:]).split(\".\") # Initially split to separate label, then separate sentences\n",
    "        for s in sentences:\n",
    "            if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                split_sentences.append(\" \".join([label, s]))\n",
    "    return split_sentences\n",
    "\n",
    "\n",
    "input_data  = unzip_data('Toys_and_Games_5.json.zip')\n",
    "labeled_data = label_data('Toys_and_Games_5.json')\n",
    "split_sentence_data = split_sentences(labeled_data)\n",
    "\n",
    "print(split_sentence_data[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6b1ff-c8df-419d-9d3e-42b3092c4366",
   "metadata": {},
   "source": [
    "Exercise: Upload Data\n",
    "\n",
    "Your first responsibility is to separate that split_sentence_data into a training_file and a validation_file. Have the training file make up 90% of the data, and have the validation file make up 10% of the data. Careful that the data doesn't overlap! (This will result in overfitting, which might result in nice validation metrics, but crummy generalization.)\n",
    "\n",
    "Using the methodology of your choice, upload these files to S3. (In practice, it's important to know how to do this through the console, programatically, and through the CLI. If you're feeling frisky, try all 3!) If you're doing this programatically, the Boto3 documentation would be a good reference. https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "\n",
    "The BUCKET will be the name of the bucket you wish to upload it to. The s3_prefix will be the name of the desired 'file-path' that you upload your file to within the bucket. For example, if you wanted to upload a file to:\n",
    "\n",
    "\"s3://example-bucket/1/2/3/example.txt\n",
    "\n",
    "The \"BUCKET\" will be 'example-bucket', and the s3_prefix would be '1/2/3'\n",
    "\n",
    "The code below shows you how to upload it programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8180a8-5157-4e42-99d6-d736c15c8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "# Note: This section implies that the bucket below has already been made and that you have access\n",
    "# to that bucket. You would need to change the bucket below to a bucket that you have write\n",
    "# premissions to. This will take time depending on your internet connection, the training file is ~ 40 mb\n",
    "\n",
    "BUCKET = \"mldeployex\"\n",
    "# s3_prefix = \"s3://mldeployex/txtdata/\"\n",
    "\n",
    "\n",
    "def cycle_data(fp, data):\n",
    "    for d in data:\n",
    "        fp.write(d + \"\\n\")\n",
    "\n",
    "def write_trainfile(split_sentence_data):\n",
    "    train_path = \"hello_blaze_train\"\n",
    "    with open(train_path, 'w') as f:\n",
    "        cycle_data(f, split_sentence_data)\n",
    "    return train_path\n",
    "\n",
    "def write_validationfile(split_sentence_data):\n",
    "    validation_path = \"hello_blaze_validation\"\n",
    "    with open(validation_path, 'w') as f:\n",
    "        cycle_data(f, split_sentence_data)\n",
    "    return validation_path \n",
    "\n",
    "def upload_file_to_s3(file_name, s3_prefix):\n",
    "    object_name = os.path.join(s3_prefix, file_name)\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, BUCKET, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "\n",
    "s3_prefix = \"s3://mldeployex/txtdata/\"\n",
    "\n",
    "# Split the data\n",
    "split_data_trainlen = int(len(split_sentence_data) * .9)\n",
    "split_data_validationlen = int(len(split_sentence_data) * .1)\n",
    "\n",
    "# Todo: write the training file\n",
    "train_path = write_trainfile(split_sentence_data[:split_data_trainlen])\n",
    "print(\"Training file written!\")\n",
    "\n",
    "# Todo: write the validation file\n",
    "validation_path = write_validationfile(split_sentence_data[split_data_trainlen:])\n",
    "print(\"Validation file written!\")\n",
    "\n",
    "upload_file_to_s3(train_path, s3_prefix)\n",
    "print(\"Train file uploaded!\")\n",
    "upload_file_to_s3(validation_path, s3_prefix)\n",
    "print(\"Validation file uploaded!\")\n",
    "\n",
    "print(\" \".join([train_path, validation_path]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e9728-ad4f-413b-b505-425442922ee1",
   "metadata": {},
   "source": [
    "Exercise: Train SageMaker Model\n",
    "\n",
    "Believe it or not, you're already almost done! Part of the appeal of SageMaker is that AWS has already done the heavy implementation lifting for you. Launch a \"BlazingText\" training job from the SageMaker console. You can do so by searching \"SageMaker\", and navigating to Training Jobs on the left hand side. After selecting \"Create Training Job\", perform the following steps:\n",
    "\n",
    "Select \"BlazingText\" from the algorithms available.\n",
    "Specify the \"file\" input mode of training.\n",
    "Under \"resource configuration\", select the \"ml.m5.large\" instance type. Specify 5 additional GBs of memory.\n",
    "Set a stopping condition for 15 minutes.\n",
    "Under hyperparameters, set \"mode\" to \"supervised\"\n",
    "Under input_data configuration, input the S3 path to your training and validation datasets under the \"train\" and \"validation\" channels. You will need to create a channel named \"validation\".\n",
    "Specify an output path in the same bucket that you uploaded training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47e0b6-2c9f-4a77-b2e8-60683c8eff53",
   "metadata": {},
   "source": [
    "## Endpoint SDK demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3244ba5-1fc8-45c3-8b6f-8f795cceea74",
   "metadata": {},
   "source": [
    "Through the SDK, we will first initiate a boto3 client. Then, we obtain the model image uri, the model artifact, and an execution role. This is used to initiate a Model object. Then, we call the deploy method, specifying what kind of instance we want and how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8667a-be94-4bfb-9324-38f4c5b9137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "image_uri = image_uris.retrieve(framework='xgboost',region='us-west-2', version='latest')\n",
    "model_data = \"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/output/xgboost-2021-08-31-23-02-30-970/output/model.tar.gz\"\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20113a23-3ba5-44b4-b548-0ecdf1ea16b9",
   "metadata": {},
   "source": [
    "Using endpoint through SDK\n",
    "\n",
    "To utilize this endpoint, you can do it programmatically through the SDK's Predictor interface. You pass in the endpoint name and your Boto3 session. Depending on the type of data, you may need to serialize the data. Serializing the data breaks the data down in such a way that it can be recreated later. An example of a predictor object and serialization is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22004d0b-4959-4d76-80fd-f0a0e310e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy the model\n",
    "deployment = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    )\n",
    "#get the endpoint name\n",
    "endpoint = deployment.endpoint_name\n",
    "\n",
    "#instantiate a Predictor\n",
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    ")\n",
    "\n",
    "#prepare one image for prediction\n",
    "predictor.serializer = IdentitySerializer(\"image/png\")\n",
    "with open(\"test_image.png\", \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "#use the predictor to make a prediction\n",
    "inference = predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834192d-3976-4fdd-adf1-7efbea4fc5e2",
   "metadata": {},
   "source": [
    "## UDACITY SageMaker Essentials: Endpoint Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb480c-2d71-4e42-bab1-fafd69f36a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0d7ce-aa63-4aaf-9380-7700ae8f70e0",
   "metadata": {},
   "source": [
    "Understanding Exercise: Preprocessing Data (again)\n",
    "\n",
    "Before we start, we're going to do preprocessing on a new set of data that we'll be evaluating on HelloBlaze. We won't keep track of the labels here, we're just seeing how we could potentially evaluate new data using an existing model. This code should be very familiar, and requires no modification. Something to note: it is getting tedious to have to manually process the data ourselves whenever we want to do something with our model. We are also doing this on our local machine. Can you think of potential limitations and dangers to the preprocessing setup we currently have? Keep this in mind when we move on to our lesson about batch-transform jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa17fdb-9931-416f-ac27-269302a03359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function below unzips the archive to the local directory. \n",
    "\n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')\n",
    "\n",
    "# Input data is a file with a single JSON object per line with the following format: \n",
    "# {\n",
    "#  \"reviewerID\": <string>,\n",
    "#  \"asin\": <string>,\n",
    "#  \"reviewerName\" <string>,\n",
    "#  \"helpful\": [\n",
    "#    <int>, (indicating number of \"helpful votes\")\n",
    "#    <int>  (indicating total number of votes)\n",
    "#  ],\n",
    "#  \"reviewText\": \"<string>\",\n",
    "#  \"overall\": <int>,\n",
    "#  \"summary\": \"<string>\",\n",
    "#  \"unixReviewTime\": <int>,\n",
    "#  \"reviewTime\": \"<string>\"\n",
    "# }\n",
    "# \n",
    "# We are specifically interested in the fields \"helpful\" and \"reviewText\"\n",
    "#\n",
    "\n",
    "def label_data(input_data):\n",
    "    labeled_data = []\n",
    "    HELPFUL_LABEL = \"__label__1\"\n",
    "    UNHELPFUL_LABEL = \"__label__2\"\n",
    "     \n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        reviewText = l_object['reviewText']\n",
    "        if total_votes != 0:\n",
    "            if helpful_votes / total_votes > .5:\n",
    "                labeled_data.append(\" \".join([HELPFUL_LABEL, reviewText]))\n",
    "            elif helpful_votes / total_votes < .5:\n",
    "                labeled_data.append(\" \".join([UNHELPFUL_LABEL, reviewText]))\n",
    "          \n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "# Labeled data is a list of sentences, starting with the label defined in label_data. \n",
    "\n",
    "def split_sentences(labeled_data):\n",
    "    new_split_sentences = []\n",
    "    for d in labeled_data:       \n",
    "        sentences = \" \".join(d.split()[1:]).split(\".\") # Initially split to separate label, then separate sentences\n",
    "        for s in sentences:\n",
    "            if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                new_split_sentences.append(s)\n",
    "    return new_split_sentences\n",
    "\n",
    "\n",
    "unzip_data('reviews_Musical_Instruments_5.json.zip')\n",
    "labeled_data = label_data('reviews_Musical_Instruments_5.json')\n",
    "new_split_sentence_data = split_sentences(labeled_data)\n",
    "\n",
    "print(new_split_sentence_data[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00ea2e-ed3c-4597-9eef-0c34c886e9c7",
   "metadata": {},
   "source": [
    "Exercise: Deploy Model\n",
    "\n",
    "Once you have your model, it's trivially easy to create an endpoint. All you need to do is initialize a \"model\" object, and call the deploy method. Fill in the method below with the proper addresses and an endpoint will be created, serving your model. Once this is done, confirm that the endpoint is live by consulting the SageMaker Console. You'll see this under \"Endpoints\" in the \"Inference\" menu on the left-hand side. If done correctly, this will take a while to get instantiated.\n",
    "\n",
    "You will need the following methods:\n",
    "\n",
    "You'll need image_uris.retrieve method to determine the image uri to get a BlazingText docker image uri https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html\n",
    "You'll need a model_data to pass the S3 location of a SageMaker model data\n",
    "You'll need to use the Model object https://sagemaker.readthedocs.io/en/stable/api/inference/model.html\n",
    "You'll need to the get execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a6729-4c41-40e2-8416-9f2196310c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# get the execution role\n",
    "role = get_execution_role()\n",
    "# get the image using the \"blazingtext\" framework and your region\n",
    "image_uri = image_uris.retrieve(framework='blazingtext', region='us-east-1', version='latest')\n",
    "# get the S3 location of a SageMaker model data\n",
    "model_data = 's3://mldeployex/12el/model_artifact/blazetxt/output/model.tar.gz'\n",
    "# define a model object\n",
    "model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
    "# deploy the model using a single instance of \"ml.m5.large\"\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7a6b6-c247-42a6-bb90-e98c5dd20bec",
   "metadata": {},
   "source": [
    "Exercise: Evaluate Data\n",
    "\n",
    "Alright, we now have an easy way to evaluate our data! You will want to interact with the endpoint using the predictor interface: https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html\n",
    "\n",
    "Predictor is not the endpoint itself, but instead is an interface that we can use to easily interact with our deployed model. Your task is to take new_split_sentence_data and evaluate it using the predictor.\n",
    "\n",
    "Note that the BlazingText supports \"application/json\" as the content-type for inference and the model expects a payload that contains a list of sentences with the key as “instances”.\n",
    "\n",
    "The method you'll need to call is highlighted below.\n",
    "\n",
    "Another recommendation: try evaluating a subset of the data before evaluating all of the data. This will make debugging significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68da06-2b46-4afe-86cd-16e6b38f0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "import json\n",
    "\n",
    "predictor = Predictor('blazingtext-2022-05-14-06-32-59-663')\n",
    "\n",
    "# load the first five reviews from new_split_sentence_data\n",
    "example_sentences = new_split_sentence_data[0:5]\n",
    "\n",
    "payload = {\"instances\": example_sentences}\n",
    "\n",
    "print(json.dumps(payload))\n",
    "\n",
    "# make predictions using the \"predict\" method. Set initial_args to {'ContentType': 'application/json'}\n",
    "predictions = json.loads(predictor.predict(json.dumps(payload), initial_args={'ContentType': 'application/json'}))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0c9f9-4249-4ebb-96dc-7ea8c42bf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.stop_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3fcea-1bc1-4112-ae00-2fc736de6810",
   "metadata": {},
   "source": [
    "## Batch Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d165f-0bdf-4c08-99a7-7ddeda71b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client(‘sagemaker’)\n",
    "response = client.create_transform_job(\n",
    "    TransformJobName=‘my_job’,\n",
    "    ModelName=‘target_model_for_inference’,\n",
    "    TransformInput={\n",
    "        ‘DataSource’: {\n",
    "            ‘S3DataSource’: {\n",
    "            ‘S3DataType’: ‘S3Prefix’,\n",
    "            ‘S3Uri’: ‘s3://mybucket/inputfolder/’\n",
    "         }\n",
    "     },\n",
    "    ‘SplitType’: ‘Line’\n",
    "    },\n",
    "    TransformInput={\n",
    "        ‘S3OutputPath’: ‘s3://mybucket/outputpath’,\n",
    "        ‘AssembleWith’: ‘Line’\n",
    "    },\n",
    "    TransformResources={\n",
    "    'InstanceType’: ‘ml.m4.xlarge’,\n",
    "    ‘InstanceCount’: 1})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5cb10-3eea-4f52-bd35-0b3b34f751ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d15b6b-5d44-4042-bca6-5c99f8ad13fa",
   "metadata": {},
   "source": [
    "## Batch Transform Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561cfacc-1729-4de9-a752-483a33b778f9",
   "metadata": {},
   "source": [
    "using SDK \n",
    "\n",
    "Programmatically, we do this operation in a similar pattern to what we’re used to. First, we’ll create a boto3 session. Then, similar to our operations on endpoints, we create a model object utilizing the image uri of Amazon’s XGBoost, our model artifact, and an execution role.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a799dc-e061-46b8-a885-995f8d2551e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating model objects\n",
    "role = get_execution_role()\n",
    "image_uri = image_uris.retrieve(framework='xgboost',region='us-west-2', version='latest')\n",
    "model_data = \"s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/output/xgboost-2021-08-31-23-02-30-970/output/model.tar.gz\"\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=model_data, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ac41f-27cf-4de6-abd7-4c8c7ba8d9dc",
   "metadata": {},
   "source": [
    "Using this model object, we create a transformer object. This is used to perform operations on data, which in this case would be inference. We specify machine type, # of machines, and the output of where we want our transformations to go.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5579f8-d26e-42a6-8858-66cea3c9c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = model.transformer(\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    output_path = batch_transform_output_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec460ae-6258-448d-9179-93f20c1f2629",
   "metadata": {},
   "source": [
    "Then, you call the transform method to do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b26a11-dc8a-47d8-a747-df1f3a69c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(\n",
    "    data=data_s3_path,\n",
    "    data_type='S3Prefix',\n",
    "    content_type='text/csv',\n",
    "    split_type='Line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf853b0-3e1b-4511-b770-0a10323049c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d241dbad-7216-4ab9-8e41-7fbfc2af06fe",
   "metadata": {},
   "source": [
    "Exercise: Preprocess (again, again) and upload to S3\n",
    "\n",
    "The cell below provides you two functions. The split_sentences preprocesses the reviews and you should be very familiar with function. Remember that the BlazingText expects a input with JSON format, the cycle_data formats the review to the following: {'source': 'THIS IS A SAMPLE SENTENCE'} and writes it into a file.\n",
    "\n",
    "Using the cell to complete the following tasks:\n",
    "\n",
    "preprecessing reviews_Musical_Instruments_5.json\n",
    "upload the file consisting of the data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f095e20-5ede-48c5-971a-96e510d4006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Todo: Input the s3 bucket\n",
    "s3_bucket = \"mldeployex\"\n",
    "\n",
    "# Todo: Input the s3 prefix\n",
    "s3_prefix = \"13el\"\n",
    "\n",
    "# Todo: Input the the file to write the data to\n",
    "file_name = \"music_instruments_reviews.txt\"\n",
    "\n",
    "# Function below unzips the archive to the local directory. \n",
    "\n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')\n",
    "\n",
    "\n",
    "def split_sentences(input_data):\n",
    "    split_sentences = []\n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        if total_votes != 0 and helpful_votes/total_votes != .5:  # Filter out same data as prior jobs. \n",
    "            reviewText = l_object['reviewText']\n",
    "            sentences = reviewText.split(\".\") \n",
    "            for s in sentences:\n",
    "                if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                    split_sentences.append(s)\n",
    "    return split_sentences\n",
    "\n",
    "# Format the data as {'source': 'THIS IS A SAMPLE SENTENCE'}\n",
    "# And write the data into a file\n",
    "def cycle_data(fp, data):\n",
    "    for d in data:\n",
    "        fp.write(json.dumps({'source':d}) + '\\n')\n",
    "\n",
    "# Todo: write a function to upload the data to s3\n",
    "def upload_file_to_s3(file_name, s3_prefix):\n",
    "    object_name = os.path.join(s3_prefix, file_name)\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response=s3_client.upload_file(file_name, s3_bucket, object_name)\n",
    "    except clientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "\n",
    "\n",
    "# Unzips file.\n",
    "unzip_data('reviews_Musical_Instruments_5.json.zip')\n",
    "\n",
    "# Todo: preprocess reviews_Musical_Instruments_5.json \n",
    "sentences = split_sentences('reviews_Musical_Instruments_5.json')\n",
    "\n",
    "# Write data to a file and upload it to s3.\n",
    "with open(file_name, 'w') as f:\n",
    "    cycle_data(f, sentences)\n",
    "\n",
    "upload_file_to_s3(file_name, s3_prefix)\n",
    "\n",
    "# Get the s3 path for the data\n",
    "batch_transform_input_path = \"s3://\" + \"/\".join([s3_bucket, s3_prefix, file_name])\n",
    "\n",
    "print(batch_transform_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61e9b4-a28c-4df5-ac38-937e34480f05",
   "metadata": {},
   "source": [
    "Exercise: Use Batch Transform to perform an inference on the dataset\n",
    "\n",
    "\n",
    "We utilize batch transform through a transformer object. Similar to how we initialized a predictor object in the last exercise, complete the code below to initialize a transformer object and launch a transform job.\n",
    "\n",
    "You will need the following:\n",
    "\n",
    "Similar to last exercise, you will need to get a BlazingText image uri from AWS. The methodology you use to do so should be identical to the last exercise.\n",
    "You will need to instantiate a \"model\" object.\n",
    "You will need to call the \"transformer\" method on the model object to create a transformer. We suggest using 1 instance of ml.m4.xlarge. If this isn't available in your region, feel free to use another instance, such as ml.m5.large\n",
    "You will need to use this transformer on the data we uploaded to s3. You will be able to do so by inserting an \"S3Prefix\" data_type and a \"application/jsonlines\" content_type, split by \"Line\".\n",
    "Consult the following documentation: https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html\n",
    "\n",
    "End-to-end, this process should take about 5 minutes on the whole dataset. While developing, consider uploading a subset of the data to s3, and evaluate on that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5be8a-9ab5-4cf0-bf1c-6dbaa6fd989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# Get the execution role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Get the image uri using the \"blazingtext\" algorithm in your region. \n",
    "\n",
    "image_uri = image_uris.retrieve(framework='blazingtext',region='us-east-1')\n",
    "\n",
    "# Get the model artifact from S3\n",
    "\n",
    "model_data = 's3://mldeployex/12el/model_artifact/blazetxt/output/model.tar.gz'\n",
    "\n",
    "# Get the s3 path for the batch transform data\n",
    "\n",
    "batch_transform_output_path = \"s3://mldeployex/l3el/batchtransform_output\"\n",
    "\n",
    "# Define a model object\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
    "\n",
    "# Define a transformer object, using a single instance ml.m4.xlarge. Specify an output path to your s3 bucket. \n",
    "\n",
    "transformer = model.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m4.xlarge',\n",
    "        output_path=batch_transform_output_path\n",
    ")\n",
    "\n",
    "# Call the transform method. Set content_type='application/jsonlines', split_type='Line'\n",
    "\n",
    "transformer.transform(\n",
    "    data=batch_transform_input_path, \n",
    "    data_type='S3Prefix',\n",
    "    content_type='application/jsonlines', \n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "transformer.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad161d88-41c5-4e58-8bfc-e67a5ca367f1",
   "metadata": {},
   "source": [
    "## Processing Job Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53d419-3367-49cc-a0b6-401fd1b2171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "Sklearn_processor = SKLearnProcessor(framework_version=string,\n",
    "    role=string,\n",
    "    instance_type=string,\n",
    "    instance_count=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1cc68e-392a-4627-b002-1045e3cda529",
   "metadata": {},
   "source": [
    "Then we start the processing job by executing the run method of the Processor object. Within the run method, we need to specify:\n",
    "\n",
    "The code we wish to use to process the data\n",
    "Input channels. (Both S3 and local path within the ProcessingInput)\n",
    "Output channels. (The local path within the ProcessingOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3925de-5a47-4d9e-82db-915420552544",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor.run(code='xgboost_process_script.py',\n",
    "                        inputs=[ProcessingInput(\n",
    "                        source='s3://sagemaker-us-west-2-565094796913/boston-xgboost-HL/train.csv',\n",
    "                        destination='/opt/ml/processing/input/data/')],\n",
    "                        outputs=[ProcessingOutput(source='/opt/ml/processing/output')]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbdb227-f1d4-4b61-b663-a16363528c7b",
   "metadata": {},
   "source": [
    "AWS Console\n",
    "\n",
    "To launch a processing job through the console, we need to specify the following\n",
    "\n",
    "A name.\n",
    "\n",
    "Desired ECR image.246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3\n",
    "\n",
    "An IAM role\n",
    "\n",
    "Computing resources, again going for the cheaper option.\n",
    "\n",
    "Entry Point: python3;/opt/ml/processing/input/code/xgboost_process_script.py\n",
    "\n",
    "Input channel\n",
    "\n",
    "    Data S3 location\n",
    "    \n",
    "    Data local path:/opt/ml/processing/input/data\n",
    "    \n",
    "    Code S3 location\n",
    "    \n",
    "    Code local path: /opt/ml/processing/input/code)\n",
    "\n",
    "Output channels\n",
    "\n",
    "    Output S3 location\n",
    "    \n",
    "    Local path:/opt/ml/processing/output\n",
    "\n",
    "Trigger for upload (End of Job)\n",
    "\n",
    "With that, we can submit the job and track it through the Processing Jobs landing page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ee1d4-44fe-4706-9dda-e7f67bb7c574",
   "metadata": {},
   "source": [
    "## UDACITY SageMaker Essentials: Processing Job Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74778dc3-8a35-4541-b191-8d546e437ac1",
   "metadata": {},
   "source": [
    "In prior exercises, we've been running and rerunning the same preprocessing job over and over again. For cleanly formatted data, it's possible to do some preprocessing utilizing batch transform. However, if slightly more sophisticated processing is needed, we would want to do so through a processing job. Finally, after beating around the bush for a few exercises, we're finally going offload the preprocessing step of our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05fb97-f575-4760-a277-76b30d273ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ddcbb-c7c4-49c9-a9fb-8af28434f6f9",
   "metadata": {},
   "source": [
    "## Preprocessing (for the final time!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb9d06-5144-44eb-9afc-4b7b9c28b599",
   "metadata": {},
   "source": [
    "The cell below should be very familiar to you by now. This cell will write the preprocessing code to a file called \"HelloBlazePreprocess.py\". This code will be utilized by AWS via a SciKitLearn processing interface to process our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb459165-e623-459c-823c-ae728826035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile HelloBlazePreprocess.py\n",
    "\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "# Function below unzips the archive to the local directory. \n",
    "\n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')\n",
    "        return input_data_zip.namelist()[0]\n",
    "\n",
    "# Input data is a file with a single JSON object per line with the following format: \n",
    "# {\n",
    "#  \"reviewerID\": <string>,\n",
    "#  \"asin\": <string>,\n",
    "#  \"reviewerName\" <string>,\n",
    "#  \"helpful\": [\n",
    "#    <int>, (indicating number of \"helpful votes\")\n",
    "#    <int>  (indicating total number of votes)\n",
    "#  ],\n",
    "#  \"reviewText\": \"<string>\",\n",
    "#  \"overall\": <int>,\n",
    "#  \"summary\": \"<string>\",\n",
    "#  \"unixReviewTime\": <int>,\n",
    "#  \"reviewTime\": \"<string>\"\n",
    "# }\n",
    "# \n",
    "# We are specifically interested in the fields \"helpful\" and \"reviewText\"\n",
    "#\n",
    "\n",
    "def label_data(input_data):\n",
    "    labeled_data = []\n",
    "    HELPFUL_LABEL = \"__label__1\"\n",
    "    UNHELPFUL_LABEL = \"__label__2\"\n",
    "     \n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        reviewText = l_object['reviewText']\n",
    "        if total_votes != 0:\n",
    "            if helpful_votes / total_votes > .5:\n",
    "                labeled_data.append(\" \".join([HELPFUL_LABEL, reviewText]))\n",
    "            elif helpful_votes / total_votes < .5:\n",
    "                labeled_data.append(\" \".join([UNHELPFUL_LABEL, reviewText]))\n",
    "          \n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "# Labeled data is a list of sentences, starting with the label defined in label_data. \n",
    "\n",
    "def split_sentences(labeled_data):\n",
    "    new_split_sentences = []\n",
    "    for d in labeled_data:\n",
    "        label = d.split()[0]        \n",
    "        sentences = \" \".join(d.split()[1:]).split(\".\") # Initially split to separate label, then separate sentences\n",
    "        for s in sentences:\n",
    "            if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                new_split_sentences.append(\" \".join([label, s]))\n",
    "    return new_split_sentences\n",
    "\n",
    "def write_data(data, train_path, test_path, proportion):\n",
    "    border_index = int(proportion * len(data))\n",
    "    train_f = open(train_path, 'w')\n",
    "    test_f = open(test_path, 'w')\n",
    "    index = 0\n",
    "    for d in data:\n",
    "        if index < border_index:\n",
    "            train_f.write(d + '\\n')\n",
    "        else:\n",
    "            test_f.write(d + '\\n')\n",
    "        index += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unzipped_path = unzip_data('/opt/ml/processing/input/Toys_and_Games_5.json.zip')\n",
    "    labeled_data = label_data(unzipped_path)\n",
    "    new_split_sentence_data = split_sentences(labeled_data)\n",
    "    write_data(new_split_sentence_data, '/opt/ml/processing/output/train/hello_blaze_train_scikit', '/opt/ml/processing/output/test/hello_blaze_test_scikit', .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644172f-9d60-424c-a8fb-037a67bcec3f",
   "metadata": {},
   "source": [
    "## Exercise: Upload unprocessed data to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d3c5d-9ebf-4aa6-9f3f-b23e2a28895b",
   "metadata": {},
   "source": [
    "No more local preprocessing! Upload the raw zipped Toys_and_Games dataset to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8d056-9a48-42f8-b5cc-5fe13880a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import boto3\n",
    "\n",
    "\n",
    "# Todo\n",
    "s3_bucket = \"mldeployex\"\n",
    "s3_prefix = \"13e1\"\n",
    "file_name = \"Toys_and_Games_5.json.zip\"\n",
    "\n",
    "def upload_file_to_s3(file_name):\n",
    "    object_name = os.path.join(s3_prefix, file_name)\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, s3_bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    \n",
    "upload_file_to_s3(file_name)\n",
    "\n",
    "source_path = \"s3://\" + \"/\".join([s3_bucket, s3_prefix, file_name])\n",
    "print(source_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40753fb0-e81f-4c0f-919e-1e0813ba6aa1",
   "metadata": {},
   "source": [
    "Exercise: Launch a processing job through the SciKitLearn interface.\n",
    "\n",
    "We'll be utilizing the SKLearnProcessor object from SageMaker to launch a processing job. Here is some information you'll need to complete the exercise:\n",
    "\n",
    "You will need to use the SKLearnProcessor object.\n",
    "You will need 1 instance of ml.m5.large. You can specify this programatically.\n",
    "You will need an execution role.\n",
    "\n",
    "You will need to call the \"run\" method on the SKLearnProcessor object.\n",
    "\n",
    "You will need to specify the preprocessing code\n",
    "the S3 path of the unprocessed data\n",
    "a 'local' directory path for the input to be downloaded into\n",
    "'local' directory paths for where you expect the output to be.\n",
    "you will need to make use of the ProcessingInput and ProcessingOutput features. Review the preprocessing code for where the output is going to go, and where it expects the input to come from.\n",
    "\n",
    "It is highly recommended that you consult the documentation to help you implement this. https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html\n",
    "Remember that, conceptually, you are creating a server that our code will be run from. This server will download data, execute code that we specify, and upload data to s3.\n",
    "\n",
    "If done successfully, you should see a processing job launch in the SageMaker console. To see it, go to the \"processing\" drop-down menu on the left-hand side and select \"processing jobs.\" Wait until the job is finished.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29059b-a103-4802-bf72-c6ee8022aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Get role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Create an SKLearnProcessor. Set framework_version='0.20.0'.\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "    role=role,\n",
    "    instance_type= 'ml.m5.large',\n",
    "    instance_count=1)\n",
    "\n",
    "# Start a run job. You will pass in as parameters the local location of the processing code, \n",
    "# a processing input object, two processing output objects. The paths that you pass in here are directories, \n",
    "# not the files themselves. Check the preprocessing code for a hint about what these directories should be. \n",
    "\n",
    "sklearn_processor.run(code= 'HelloBlazePreprocess.py', # preprocessing code\n",
    "                      inputs=[ProcessingInput(\n",
    "                          source = source_path, # the S3 path of the unprocessed data\n",
    "                          destination='/opt/ml/processing/input' , # a 'local' directory path for the input to be downloaded into\n",
    "                      )],\n",
    "                      outputs=[ProcessingOutput(source='/opt/ml/processing/output/train' ),# a 'local' directory path for where you expect the output for train data to be\n",
    "                               ProcessingOutput(source='/opt/ml/processing/output/test' )]) # a 'local' directory path for where you expect the output for test data to be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a8faa-f38a-4677-84f7-db226b1305ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor.jobs[-1].describe() #to check the output path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6a0c5-e08f-46e0-8853-5b8e3c115cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
